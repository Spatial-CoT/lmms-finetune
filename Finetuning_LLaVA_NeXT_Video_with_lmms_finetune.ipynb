{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxVDutGjrUvx"
      },
      "source": [
        "## About\n",
        "\n",
        "This colab notebook showcases how easy it is to finetune a large multimodal model (LMM; or multi-modal LLM), with the codebase of **[lmms-finetune](https://github.com/zjysteven/lmms-finetune)**. Specially we will finetune the powerful LLaVA-NeXT-Video model on some ego4d video clips to generate detailed video captions.\n",
        "\n",
        "This notebook is written on 2024/07/30. lmms-finetune is undergoing active development. So for more details and updates, please don't hesistate to check out https://github.com/zjysteven/lmms-finetune\n",
        "\n",
        "*Note:* Running this notebook requires sufficient GPU resource (A100 would be the best but L4 also works)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jyYF4tFqLwo",
        "outputId": "a78d3f60-55ae-418e-a579-2c013678ab5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'lmms-finetune'...\n",
            "remote: Enumerating objects: 400, done.\u001b[K\n",
            "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (310/310), done.\u001b[K\n",
            "remote: Total 400 (delta 182), reused 286 (delta 87), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (400/400), 13.22 MiB | 19.88 MiB/s, done.\n",
            "Resolving deltas: 100% (182/182), done.\n",
            "/content/lmms-finetune\n",
            "Collecting transformers@ git+https://github.com/huggingface/transformers.git (from -r requirements.txt (line 3))\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-install-rt2g_ftv/transformers_382087e6e4a643908aa49bb8e0653231\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-rt2g_ftv/transformers_382087e6e4a643908aa49bb8e0653231\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 62c60a30181a65e1a3a7f19c3055a240a6a21335\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.18.1+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.32.1)\n",
            "Collecting peft (from -r requirements.txt (line 5))\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting deepspeed (from -r requirements.txt (line 6))\n",
            "  Downloading deepspeed-0.14.4.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tiktoken (from -r requirements.txt (line 7))\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (3.7.1)\n",
            "Collecting bitsandbytes (from -r requirements.txt (line 9))\n",
            "  Downloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting av (from -r requirements.txt (line 10))\n",
            "  Downloading av-12.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.1.99)\n",
            "Collecting wandb (from -r requirements.txt (line 12))\n",
            "  Downloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (3.20.3)\n",
            "Collecting transformers_stream_generator (from -r requirements.txt (line 16))\n",
            "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 3)) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 3)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 3)) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 3)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 3)) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 3)) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 4)) (5.9.5)\n",
            "Collecting hjson (from deepspeed->-r requirements.txt (line 6))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ninja (from deepspeed->-r requirements.txt (line 6))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting nvidia-ml-py (from deepspeed->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_ml_py-12.555.43-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 6)) (9.0.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 12)) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 12))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 12))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 12)) (4.2.2)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 12))\n",
            "  Downloading sentry_sdk-2.11.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting setproctitle (from wandb->-r requirements.txt (line 12))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 12)) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 12)) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 12))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 3)) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed->-r requirements.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed->-r requirements.txt (line 6)) (2.20.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 12))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m156.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-12.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.11.0-py2.py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.6/303.6 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-12.555.43-py3-none-any.whl (39 kB)\n",
            "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: transformers, deepspeed, transformers_stream_generator\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.44.0.dev0-py3-none-any.whl size=9410983 sha256=f95b7a72d2e2367b5d82578ac3078fd69aaa7ebe912d9556bb5f9d67b2fc22e9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zldm8aa2/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.14.4-py3-none-any.whl size=1445520 sha256=fbf868f543227468ead74f3e24eb95dbe19330d862193af993ba76c9cc23cd85\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/bc/a3/608e90bbb301848b78fd75d24d6d43ba3074de968fc0e397ac\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12424 sha256=1ce66f27bc0664fcd94e18891281665ca1b4410c59756bc303311fb82ea0ef33\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/4a/90/140f7b67d125906f6a165f38aad212ecb4a695ad0d87582437\n",
            "Successfully built transformers deepspeed transformers_stream_generator\n",
            "Installing collected packages: nvidia-ml-py, ninja, hjson, smmap, setproctitle, sentry-sdk, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, docker-pycreds, av, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gitdb, nvidia-cusolver-cu12, gitpython, wandb, transformers, transformers_stream_generator, deepspeed, bitsandbytes, peft\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "Successfully installed av-12.3.0 bitsandbytes-0.43.2 deepspeed-0.14.4 docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 hjson-3.1.0 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py-12.555.43 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 peft-0.12.0 sentry-sdk-2.11.0 setproctitle-1.3.3 smmap-5.0.1 tiktoken-0.7.0 transformers-4.44.0.dev0 transformers_stream_generator-0.0.5 wandb-0.17.5\n"
          ]
        }
      ],
      "source": [
        "# clone the codebase\n",
        "!git clone https://github.com/zjysteven/lmms-finetune\n",
        "\n",
        "# install dependencies\n",
        "%cd lmms-finetune\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq5qjXLNulrq"
      },
      "source": [
        "## Step 0: Check if the target model is supported"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M_6paoWtkzE",
        "outputId": "359f6bb8-d1e9-421d-b0a6-c283ee28bcc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Supported models:\n",
            "  Model ID                      : HuggingFace Path\n",
            "  ------------------------------------------------\n",
            "  llava-1.5-7b                  : llava-hf/llava-1.5-7b-hf\n",
            "  llava-1.5-13b                 : llava-hf/llava-1.5-13b-hf\n",
            "  llava-1.6-vicuna-7b           : llava-hf/llava-v1.6-vicuna-7b-hf\n",
            "  llava-1.6-vicuna-13b          : llava-hf/llava-v1.6-vicuna-13b-hf\n",
            "  llava-next-video-7b           : llava-hf/LLaVA-NeXT-Video-7B-hf\n",
            "  llava-next-video-7b-32k       : llava-hf/LLaVA-NeXT-Video-7B-32K-hf\n",
            "  llava-next-video-34b          : llava-hf/LLaVA-NeXT-Video-34B-hf\n",
            "  llava-interleave-qwen-0.5b    : llava-hf/llava-interleave-qwen-0.5b-hf\n",
            "  llava-interleave-qwen-7b      : llava-hf/llava-interleave-qwen-7b-hf\n",
            "  llava-onevision-0.5b-ov       : llava-hf/llava-onevision-qwen2-0.5b-ov-hf\n",
            "  llava-onevision-7b-ov         : llava-hf/llava-onevision-qwen2-7b-ov-hf\n",
            "  llava-onevision-72b-ov        : llava-hf/llava-onevision-qwen2-72b-ov-hf\n",
            "  qwen-vl-chat                  : Qwen/Qwen-VL-Chat\n",
            "  phi3-v                        : microsoft/Phi-3-vision-128k-instruct\n",
            "  qwen2-vl-2b-instruct          : Qwen/Qwen2-VL-2B-Instruct\n",
            "  qwen2-vl-7b-instruct          : Qwen/Qwen2-VL-7B-Instruct\n",
            "  qwen2-vl-72b-instruct         : Qwen/Qwen2-VL-72B-Instruct\n",
            "  qwen2.5-vl-3b-instruct        : Qwen/Qwen2.5-VL-3B-Instruct\n",
            "  qwen2.5-vl-7b-instruct        : Qwen/Qwen2.5-VL-7B-Instruct\n",
            "  qwen2.5-vl-72b-instruct       : Qwen/Qwen2.5-VL-72B-Instruct\n",
            "  llama-3.2-11b-vision-instruct : meta-llama/Llama-3.2-11B-Vision-Instruct\n",
            "  llama-3.2-90b-vision-instruct : meta-llama/Llama-3.2-90B-Vision-Instruct\n"
          ]
        }
      ],
      "source": [
        "!python supported_models.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooMajr9gu6j3"
      },
      "source": [
        "You can see from the displayed information which models are supported by lmms-finetune. Here we will finetune `LLaVA-NeXT-Video-7B` whose model ID is `llava-next-video-7b`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xjIVTF4vXO1"
      },
      "source": [
        "## Step 1: Prepare finetuning data\n",
        "\n",
        "For more details please see https://github.com/zjysteven/lmms-finetune. Essentially lmms-finetune expects a human-friendly format of json file which couldn't be more readable in my opinion. Below we show the data we will be using later to give you a sense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1yeMoNSu2Oh",
        "outputId": "1e104d13-fb5f-45dd-a2bb-3d7516e1fdd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'system_prompt': '', 'video': '/data_new/spatial/Training/videos/scannet/datasets/scans_videos/scene0191_01.mp4', 'conversations': [{'from': 'human', 'value': \"<video>These are frames of a video.\\nIf I am standing by the table and facing the door, is the backpack to my left, right, or back?\\nAn object is to my back if I would have to turn at least 135 degrees in order to face it.\\nOptions:\\nA. left\\nB. back\\nC. right\\nAnswer with the option's letter from the given choices directly.\"}, {'from': 'gt', 'value': 'A'}]}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "data = json.load(open(\"/home/rilyn/project-files/test/vsi-ft-dataset/data/qa_pairs/all_qa/fixed_dataset.json\", \"r\"))\n",
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d249a17f7d1440918f96cb02001d084a",
            "a67fe0f0f21048749940f3df48e44eef",
            "14bef7360ba44509be50d7b5af77f8a3",
            "29524b0bfc6748a59c474228e8e10bf3",
            "fdc6f9e4dfa34836870bea6afbb7c441",
            "cd80bf58df454a6a8dc45285ac03e2f4",
            "56c9d85de11c47e98966562666826d4a",
            "559939daaa40410a842934de61d14d3c",
            "53a8acbe80e14b57b39661d9f8169d5d",
            "e11ea97d34c1402f83f61c6b88e079ff",
            "bd5510db9e904f298e9e720a9ff3bf39"
          ]
        },
        "id": "G5AXpuKowAHx",
        "outputId": "a997c4e2-2a8c-44c0-ccb8-98654a5afa3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d249a17f7d1440918f96cb02001d084a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ego4d_videos_4.zip:   0%|          | 0.00/14.4G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  example_data/videos/zip_folder/ego4d/ego4d_videos_4.zip\n",
            "  inflating: example_data/videos/ego4d/007cb0df-4f4f-4810-b246-8ba6639f53e1.mp4  \n",
            "  inflating: example_data/videos/ego4d/0219ad48-8f54-4f61-b22f-4d1e8173e584.mp4  \n",
            "  inflating: example_data/videos/ego4d/019a251b-f3fb-4fc9-82dc-ca1b9fe42e12.mp4  \n",
            "  inflating: example_data/videos/ego4d/029532a0-3b50-457d-a790-f9dcabf93101.mp4  \n",
            "  inflating: example_data/videos/ego4d/042d40a2-f450-4322-8d4e-e5d5f8864475.mp4  \n",
            "  inflating: example_data/videos/ego4d/056db3f1-f957-46c8-b16b-c8fce22e78f9.mp4  \n",
            "  inflating: example_data/videos/ego4d/0386e502-b034-4cb3-ab3e-f44c154f18dc.mp4  \n",
            "  inflating: example_data/videos/ego4d/07309684-1f6e-4977-ab74-f3e63c361f36.mp4  \n",
            "  inflating: example_data/videos/ego4d/06899020-dca3-4612-92cf-3427d6dac6e3.mp4  \n",
            "  inflating: example_data/videos/ego4d/01d32889-b5c3-4b2f-9a37-f751b7f818d4.mp4  \n",
            "  inflating: example_data/videos/ego4d/045451d6-2916-4c07-8e47-7cfdaa579086.mp4  \n",
            "  inflating: example_data/videos/ego4d/023bf95e-28de-43b4-a43f-720edba667a5.mp4  \n",
            "  inflating: example_data/videos/ego4d/002c3b5c-ed86-4af3-99a1-4b497b7c8a86.mp4  \n",
            "  inflating: example_data/videos/ego4d/0242eca2-9df5-41e7-917e-271e28eacd39.mp4  \n",
            "  inflating: example_data/videos/ego4d/0031d268-818c-4ec4-a804-935be610a61a.mp4  \n",
            "  inflating: example_data/videos/ego4d/025c63c9-a371-4de7-87a9-ef3949040e8c.mp4  \n",
            "  inflating: example_data/videos/ego4d/053398fd-fcc4-47fc-af69-99366b13a505.mp4  \n",
            "  inflating: example_data/videos/ego4d/0538719e-78e5-45dd-a811-f7d32ce1d02b.mp4  \n",
            "  inflating: example_data/videos/ego4d/07b357ce-0bce-4d5e-b7ab-c16f1d4c7de6.mp4  \n",
            "  inflating: example_data/videos/ego4d/06456897-960d-4d0c-8ce2-cd50a5a57bc3.mp4  \n",
            "  inflating: example_data/videos/ego4d/07eacb45-2a2b-40fb-b60d-2acb7e108f34.mp4  \n",
            "  inflating: example_data/videos/ego4d/0b703793-134a-4889-bc40-c19328d8f7cd.mp4  \n",
            "  inflating: example_data/videos/ego4d/0aed42ed-ed8c-4dfb-9a7f-bd47cc792633.mp4  \n",
            "  inflating: example_data/videos/ego4d/0afbf08b-09a5-4c15-a85a-5ebf88866443.mp4  \n",
            "  inflating: example_data/videos/ego4d/0a3097fc-baed-4d11-a4c9-30f07eb91af6.mp4  \n",
            "  inflating: example_data/videos/ego4d/0d271871-c8ba-4249-9434-d39ce0060e58.mp4  \n",
            "  inflating: example_data/videos/ego4d/0ddb9f0e-5f3d-4fba-9367-ba0db225f449.mp4  \n",
            "  inflating: example_data/videos/ego4d/0836e1a4-11e6-4b31-bd39-f8e083fdadb3.mp4  \n",
            "  inflating: example_data/videos/ego4d/0de39e75-fb19-47d4-818d-fff874b05ab9.mp4  \n",
            "  inflating: example_data/videos/ego4d/0e69b2a8-6579-4bbe-837a-298ed4d3ccfa.mp4  \n",
            "  inflating: example_data/videos/ego4d/0a4c05df-f522-4a33-862f-0d89824e7300.mp4  \n",
            "  inflating: example_data/videos/ego4d/0e7620e8-f1b5-4093-aff9-d73ab0039e08.mp4  \n",
            "  inflating: example_data/videos/ego4d/0e0a1ed9-d7ca-4da0-9e65-b8096cba966e.mp4  \n",
            "  inflating: example_data/videos/ego4d/09fe2f2d-7d90-4d7d-9dcd-baec4704f4cc.mp4  \n",
            "  inflating: example_data/videos/ego4d/0869732c-5228-4a28-9c5a-467a5ebef78c.mp4  \n",
            "  inflating: example_data/videos/ego4d/09711212-58f8-4c28-8de4-92f88376db4f.mp4  \n",
            "  inflating: example_data/videos/ego4d/0d807e15-cf55-4038-83c4-d064a9ec2b46.mp4  \n",
            "  inflating: example_data/videos/ego4d/08bd7380-821b-45b3-8fc7-01a0c6ed40ad.mp4  \n",
            "  inflating: example_data/videos/ego4d/0c0d388e-4449-473f-9f0c-755916a63c6c.mp4  \n",
            "  inflating: example_data/videos/ego4d/0a6d4809-352b-44ca-9bef-ead01fd9c7f5.mp4  \n",
            "  inflating: example_data/videos/ego4d/0c0ec306-2554-42fe-a744-cd1fec78f689.mp4  \n",
            "  inflating: example_data/videos/ego4d/0a76a002-ec78-4f91-b4b1-0c9f4a9ab8e5.mp4  \n",
            "  inflating: example_data/videos/ego4d/11663dd8-aa9d-429d-80e2-2807b341350e.mp4  \n",
            "  inflating: example_data/videos/ego4d/0c163d16-8c47-4773-a25f-2ee57ce9ab87.mp4  \n",
            "  inflating: example_data/videos/ego4d/0e5c7293-c782-4931-ae12-4650792651bd.mp4  \n",
            "  inflating: example_data/videos/ego4d/1397f797-0fac-40a6-abd5-a4c4cdb9663d.mp4  \n",
            "  inflating: example_data/videos/ego4d/17d79cf8-1924-4566-b8a2-54e41f14b5ea.mp4  \n",
            "  inflating: example_data/videos/ego4d/13a15553-2fdb-4d04-a7b4-2efc33538525.mp4  \n",
            "  inflating: example_data/videos/ego4d/14a3d52a-e66c-4a4c-8f71-d86f7199993c.mp4  \n",
            "  inflating: example_data/videos/ego4d/0fedfa42-bfda-47c1-8f59-2c91b2c229fe.mp4  \n",
            "  inflating: example_data/videos/ego4d/14a81c5c-4e30-43b1-808d-2bcac0b9eeb1.mp4  \n",
            "  inflating: example_data/videos/ego4d/15440074-6cd7-479c-b308-4d68bf813cc1.mp4  \n",
            "  inflating: example_data/videos/ego4d/1652a22c-c350-4c2c-9e4c-71e055d1a268.mp4  \n",
            "  inflating: example_data/videos/ego4d/177bf424-0775-46f7-815a-f4645baa3118.mp4  \n",
            "  inflating: example_data/videos/ego4d/130e4f24-c55c-4d09-a1fc-7d9198ae1030.mp4  \n",
            "  inflating: example_data/videos/ego4d/170fb6c9-8550-4e82-bb9a-4ad574c0f0fa.mp4  \n",
            "  inflating: example_data/videos/ego4d/105d3303-8e2d-4c20-96ff-e9a8ff325109.mp4  \n",
            "  inflating: example_data/videos/ego4d/1b9ed208-7309-4ef5-8dec-3c9b0f482a14.mp4  \n",
            "  inflating: example_data/videos/ego4d/1aa28dec-e81d-4a76-b808-b804efaae529.mp4  \n",
            "  inflating: example_data/videos/ego4d/1a12a86b-28d0-4756-8df8-7cc3bac74a2d.mp4  \n",
            "  inflating: example_data/videos/ego4d/1cd5c2fd-d756-4bd8-af39-4e880aa69ef8.mp4  \n",
            "  inflating: example_data/videos/ego4d/1de259f4-6130-4c10-a83c-e2a6b78c9d34.mp4  \n",
            "  inflating: example_data/videos/ego4d/1def75d5-bd39-4638-8b3d-a4e1c2ceefeb.mp4  \n",
            "  inflating: example_data/videos/ego4d/158629ec-b436-4edb-bcdf-60b4fed8674d.mp4  \n",
            "  inflating: example_data/videos/ego4d/1efb20de-7aca-4798-a7c2-0e4dc34fb687.mp4  \n",
            "  inflating: example_data/videos/ego4d/1d6f9a1c-a6fd-4273-b0cf-a2035dcedb1f.mp4  \n",
            "  inflating: example_data/videos/ego4d/196e0e8c-f29f-48de-8e1e-ce52c2e76641.mp4  \n",
            "  inflating: example_data/videos/ego4d/1b3c0015-a687-4237-abb5-cde1bfd1e85f.mp4  \n",
            "  inflating: example_data/videos/ego4d/20474be2-080f-48bd-a30c-871b9664e806.mp4  \n",
            "  inflating: example_data/videos/ego4d/198a7531-64d0-4bb4-ad52-042a0f0a0705.mp4  \n",
            "  inflating: example_data/videos/ego4d/1a7b9b5d-499a-454e-aaee-3758718fb5f2.mp4  \n",
            "  inflating: example_data/videos/ego4d/1e85d8b5-5ca8-4bbf-be51-21741ac8a694.mp4  \n",
            "  inflating: example_data/videos/ego4d/1c8e99ac-0721-4eb9-9a40-620e48e0c85a.mp4  \n",
            "  inflating: example_data/videos/ego4d/1ea00092-9ec8-4bcd-89bb-ee0e76da7f59.mp4  \n",
            "  inflating: example_data/videos/ego4d/1f371678-00ff-47d2-94cf-b6a16597da7a.mp4  \n"
          ]
        }
      ],
      "source": [
        "# download the video clips we will be using\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "hf_hub_download(\n",
        "    \"ShareGPT4Video/ShareGPT4Video\",\n",
        "    \"zip_folder/ego4d/ego4d_videos_4.zip\",\n",
        "    repo_type=\"dataset\",\n",
        "    local_dir=\"./example_data/videos\"\n",
        ")\n",
        "\n",
        "!unzip example_data/videos/zip_folder/ego4d/ego4d_videos_4.zip -d example_data/videos/ego4d\n",
        "!rm example_data/videos/zip_folder/ego4d/ego4d_videos_4.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5nvp1PSzYxn"
      },
      "source": [
        "## Step 2: Finetuning!\n",
        "\n",
        "Here we will be using LORA to finetune the LLM part of the LMM. The running script has been prepared for you (see `example_scripts/example_video.sh`). Note that lmms-finetune also supports finetuning the vision encoder and projector. You can explore these options by looking at the bash script, where there are arguments that configure these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Fm8bo10y0DG",
        "outputId": "35285c22-f40d-4c4d-aad1-0bb9ad9b99c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "[2025-01-30 21:11:49,017] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.6\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.2.0), only 1.0.0 is known to be compatible\n",
            "/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, input, weight, bias=None):\n",
            "/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "[2025-01-30 21:11:49,866] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2025-01-30 21:11:49,866] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "Loading model, tokenizer, processor...\n",
            "[2025-01-30 21:11:51,416] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 687, num_elems = 7.06B\n",
            "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:03<00:00,  1.11s/it]\n",
            "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n",
            "Vision encoder is freezed... including:\n",
            "\tvision_tower\n",
            "Vision projector is freezed... including:\n",
            "\tmulti_modal_projector\n",
            "Other multimodal component is freezed... including:\n",
            "\timage_newline\n",
            "\tvision_resampler\n",
            "LoRA for LLM enabled...\n",
            "Trainable parameters:\n",
            "\tbase_model.model.language_model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.22.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.23.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.24.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.25.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.26.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.27.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.28.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.29.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.30.mlp.down_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.self_attn.o_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.mlp.gate_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.mlp.gate_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.mlp.up_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.mlp.up_proj.lora_B.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.mlp.down_proj.lora_A.default.weight\n",
            "\tbase_model.model.language_model.model.layers.31.mlp.down_proj.lora_B.default.weight\n",
            "Loading data...\n",
            "Parameter Offload: Total persistent parameters: 12137472 in 665 params\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrilyn-han\u001b[0m (\u001b[33mrilyn-han-yale-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/rilyn/project-files/01-pj-moog/moog-pipeline-ft/lmms-finetune/wandb/run-20250130_211200-kwnb7nuu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllava-next-video-7b_lora-True_qlora-False\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rilyn-han-yale-university/lmms-ft\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rilyn-han-yale-university/lmms-ft/runs/kwnb7nuu\u001b[0m\n",
            "  0%|                                                | 0/107345 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/home/rilyn/project-files/01-pj-moog/moog-pipeline-ft/lmms-finetune/train.py\", line 201, in <module>\n",
            "    train()\n",
            "  File \"/home/rilyn/project-files/01-pj-moog/moog-pipeline-ft/lmms-finetune/train.py\", line 194, in train\n",
            "    trainer.train()\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/transformers/trainer.py\", line 2052, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/transformers/trainer.py\", line 2345, in _inner_training_loop\n",
            "    for step, inputs in enumerate(epoch_iterator):\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/accelerate/data_loader.py\", line 563, in __iter__\n",
            "    current_batch = next(dataloader_iter)\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            "    return self._process_data(data)\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/_utils.py\", line 733, in reraise\n",
            "    raise exception\n",
            "AssertionError: Caught AssertionError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/home/rilyn/project-files/01-pj-moog/moog-pipeline-ft/lmms-finetune/datasets.py\", line 131, in __getitem__\n",
            "    assert conv[\"from\"] == (self.user_key if i % 2 == 0 else self.assistant_key), \"Invalid conversation\"\n",
            "AssertionError: Invalid conversation\n",
            "\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/home/rilyn/project-files/01-pj-moog/moog-pipeline-ft/lmms-finetune/train.py\", line 201, in <module>\n",
            "[rank0]:     train()\n",
            "[rank0]:   File \"/home/rilyn/project-files/01-pj-moog/moog-pipeline-ft/lmms-finetune/train.py\", line 194, in train\n",
            "[rank0]:     trainer.train()\n",
            "[rank0]:   File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/transformers/trainer.py\", line 2052, in train\n",
            "[rank0]:     return inner_training_loop(\n",
            "[rank0]:   File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/transformers/trainer.py\", line 2345, in _inner_training_loop\n",
            "[rank0]:     for step, inputs in enumerate(epoch_iterator):\n",
            "[rank0]:   File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/accelerate/data_loader.py\", line 563, in __iter__\n",
            "[rank0]:     current_batch = next(dataloader_iter)\n",
            "[rank0]:   File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "[rank0]:     data = self._next_data()\n",
            "[rank0]:   File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            "[rank0]:     return self._process_data(data)\n",
            "[rank0]:   File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "[rank0]:     data.reraise()\n",
            "[rank0]:   File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/_utils.py\", line 733, in reraise\n",
            "[rank0]:     raise exception\n",
            "[rank0]: AssertionError: Caught AssertionError in DataLoader worker process 0.\n",
            "[rank0]: Original Traceback (most recent call last):\n",
            "[rank0]:   File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "[rank0]:   File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "[rank0]:   File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "[rank0]:   File \"/home/rilyn/project-files/01-pj-moog/moog-pipeline-ft/lmms-finetune/datasets.py\", line 131, in __getitem__\n",
            "[rank0]:     assert conv[\"from\"] == (self.user_key if i % 2 == 0 else self.assistant_key), \"Invalid conversation\"\n",
            "[rank0]: AssertionError: Invalid conversation\n",
            "\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mllava-next-video-7b_lora-True_qlora-False\u001b[0m at: \u001b[34mhttps://wandb.ai/rilyn-han-yale-university/lmms-ft/runs/kwnb7nuu\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250130_211200-kwnb7nuu/logs\u001b[0m\n",
            "E0130 21:12:08.224000 915033 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 915101) of binary: /home/rilyn/anaconda3/envs/lmms-finetune/bin/python\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/bin/torchrun\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/distributed/run.py\", line 918, in main\n",
            "    run(args)\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/distributed/run.py\", line 909, in run\n",
            "    elastic_launch(\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/home/rilyn/anaconda3/envs/lmms-finetune/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "train.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-01-30_21:12:08\n",
            "  host      : opa-1\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 915101)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!bash example_scripts/example_video.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9_kCH3U8qn2"
      },
      "source": [
        "## Step 3: Inference\n",
        "\n",
        "The finetuned model is saved locally under `checkpoints/`. In this example the checkpoint folder is `checkpoints/llava-next-video-7b_lora-True_qlora-False`. To perform inference, the key is to correctly load the model, which is almost exactly the same as how you would typically load a huggingface model (see [inference.md](https://github.com/zjysteven/lmms-finetune/blob/main/docs/inference.md) for details). After loading the model, the inference is exactly the same as the original model. The way to inference is often documented in the huggingface model card. For example for `LLaVA-NeXT-Video`, you can find it here https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "a5261604be104e0981894d312fbb2f32",
            "af5eff91ee7441b5a53890f363e1bad4",
            "2299478f9c1240f1833dbf0e7e9935bd",
            "a054f1a4c4d24039917fc38006d17fa4",
            "5ea357a385ce4d75a3ba03d2c12c7333",
            "eca46e5c76734f76ab5b35be4a5d5c68",
            "4c9ff75fd48547fd93f61ebb13dc0605",
            "d50bbac67ddd49d29e919db12443e6d3",
            "7a61eb2734c44f3b89db68e3c47bf695",
            "e1f9b273f94147b1b2c4bf9432193072",
            "2137580d9c2a467491aeafb3b5cecce9",
            "5705b6cf22fd427fa3355dce6397a688",
            "45a9f7b861214740b42a2b8f877c5c64",
            "33ee591bba8f4f73ae0a10ce9579a6b3",
            "aa493bb732ba45c2923f96c69fa0521c",
            "7155393cd4074d7fbf0d8f928926e5be",
            "4d718ee974044ea38de5210962237e60",
            "df0034bb5bba47ffa1e5812a45a16da6",
            "8bf3b5cfa7824aa4a7f9aaaaa9d8ed00",
            "8b682914d5804238b8f35859c120f106",
            "165f599c8b824cf2bc28541c9cf9c9b3",
            "be613a10608b46609547210c8f62f271"
          ]
        },
        "id": "Z2YliC160Avu",
        "outputId": "8d78f202-f7fe-4209-caae-949dc4205a9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type llava_next to instantiate a model of type llava_next_video. This is not supported for all configurations of models and can yield errors.\n",
            "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5261604be104e0981894d312fbb2f32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type llava_next to instantiate a model of type llava_next_video. This is not supported for all configurations of models and can yield errors.\n",
            "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USER: \n",
            "Please provide a detailed description of the video. ASSISTANT: This is a video featuring a person wearing a flowered garment that has many different patterns throughout. The person appears to be sewing or adjusting the garment, with various tools and materials scattered around. As the camera zooms in, the person shows the different patterns on closer parts of their clothing. The person shows off the intricate details of the design and colors of the garment's pattern. The focus is on the fine details of the garment, showcasing the craftsmanship behind it.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5705b6cf22fd427fa3355dce6397a688",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USER: \n",
            "Please provide a detailed description of the video. ASSISTANT: In the video, we see a person with a right-handed, right-dominant hand sewing skillfully, creating a series of stiches that secure a pattern or decorative element to a piece of fabric. They hold the fabric with their left hand. To the right of the sewing frame, there are tools and accessories such as a scissors and a needle, which are likely used for cutting and managing threads, respectively. The person is wearing a watch and a bracelet, and a black handbag can be seen close by, suggesting they may be in a personal or domestic setting. The sewing area is bright, well-lit, and we can see the background is a white wall with some decorative elements. On the right side of the frame, we can also notice a few personal items like a cellphone and a paperweight resting on what appears to be a desk or workspace. The camera angle and lighting focus on the sewing process allow us to see the intricate handiwork and techniques involved in the task, although some background details may be less clear.\n"
          ]
        }
      ],
      "source": [
        "import av\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n",
        "\n",
        "\n",
        "def inference(model, processor, video_path):\n",
        "    def read_video_pyav(container, indices):\n",
        "        '''\n",
        "        Decode the video with PyAV decoder.\n",
        "        Args:\n",
        "            container (`av.container.input.InputContainer`): PyAV container.\n",
        "            indices (`List[int]`): List of frame indices to decode.\n",
        "        Returns:\n",
        "            result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
        "        '''\n",
        "        frames = []\n",
        "        container.seek(0)\n",
        "        start_index = indices[0]\n",
        "        end_index = indices[-1]\n",
        "        for i, frame in enumerate(container.decode(video=0)):\n",
        "            if i > end_index:\n",
        "                break\n",
        "            if i >= start_index and i in indices:\n",
        "                frames.append(frame)\n",
        "        return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
        "\n",
        "\n",
        "    # define a chat histiry and use `apply_chat_template` to get correctly formatted prompt\n",
        "    # Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\", \"video\")\n",
        "    conversation = [\n",
        "        {\n",
        "\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"Please provide a detailed description of the video.\"},\n",
        "                {\"type\": \"video\"},\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
        "\n",
        "    container = av.open(video_path)\n",
        "\n",
        "    # sample uniformly 8 frames from the video, which is the number of frames used in training\n",
        "    total_frames = container.streams.video[0].frames\n",
        "    indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
        "    clip = read_video_pyav(container, indices)\n",
        "    inputs_video = processor(text=prompt, videos=clip, padding=True, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    output = model.generate(**inputs_video, max_new_tokens=512, do_sample=True)\n",
        "    print(processor.decode(output[0], skip_special_tokens=True))\n",
        "\n",
        "\n",
        "processor = LlavaNextVideoProcessor.from_pretrained(\n",
        "    \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n",
        ")\n",
        "\n",
        "# this is an evaluation video that hasn't been used in training\n",
        "video_path = \"./example_data/videos/ego4d/1e85d8b5-5ca8-4bbf-be51-21741ac8a694.mp4\"\n",
        "\n",
        "# the original model before finetuning\n",
        "# we load and inference with it just for comparison\n",
        "old_model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
        "    \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ").to(0)\n",
        "inference(old_model, processor, video_path)\n",
        "del old_model\n",
        "\n",
        "# the new model after finetuning\n",
        "# notice that it's exactly the same as before\n",
        "# (unless you used Q-LoRA training; see https://github.com/zjysteven/lmms-finetune/blob/main/docs/inference.md)\n",
        "new_model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
        "    \"./checkpoints/llava-next-video-7b_lora-True_qlora-False\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ").to(0)\n",
        "inference(new_model, processor, video_path)\n",
        "del new_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYkQ03eI_EvW"
      },
      "source": [
        "old model's description of the video:\n",
        "\n",
        "- This is a video featuring a person wearing a flowered garment that has many different patterns throughout. The person appears to be sewing or adjusting the garment, with various tools and materials scattered around. As the camera zooms in, the person shows the different patterns on closer parts of their clothing. The person shows off the intricate details of the design and colors of the garment's pattern. The focus is on the fine details of the garment, showcasing the craftsmanship behind it.\n",
        "\n",
        "new model's description of the video:\n",
        "\n",
        "- In the video, we see a person with a right-handed, right-dominant hand sewing skillfully, creating a series of stiches that secure a pattern or decorative element to a piece of fabric. They hold the fabric with their left hand. To the right of the sewing frame, there are tools and accessories such as a scissors and a needle, which are likely used for cutting and managing threads, respectively. The person is wearing a watch and a bracelet, and a black handbag can be seen close by, suggesting they may be in a personal or domestic setting. The sewing area is bright, well-lit, and we can see the background is a white wall with some decorative elements. On the right side of the frame, we can also notice a few personal items like a cellphone and a paperweight resting on what appears to be a desk or workspace. The camera angle and lighting focus on the sewing process allow us to see the intricate handiwork and techniques involved in the task, although some background details may be less clear.\n",
        "\n",
        "We see that the new model gives more detailed description, which indicates that our finetuning works since the training data is from ShareGPT4Video which features exactly detailed and long description of videos."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lmms-finetune",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14bef7360ba44509be50d7b5af77f8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_559939daaa40410a842934de61d14d3c",
            "max": 14424286442,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53a8acbe80e14b57b39661d9f8169d5d",
            "value": 14424286442
          }
        },
        "165f599c8b824cf2bc28541c9cf9c9b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2137580d9c2a467491aeafb3b5cecce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2299478f9c1240f1833dbf0e7e9935bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d50bbac67ddd49d29e919db12443e6d3",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a61eb2734c44f3b89db68e3c47bf695",
            "value": 3
          }
        },
        "29524b0bfc6748a59c474228e8e10bf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e11ea97d34c1402f83f61c6b88e079ff",
            "placeholder": "​",
            "style": "IPY_MODEL_bd5510db9e904f298e9e720a9ff3bf39",
            "value": " 14.4G/14.4G [01:35&lt;00:00, 184MB/s]"
          }
        },
        "33ee591bba8f4f73ae0a10ce9579a6b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bf3b5cfa7824aa4a7f9aaaaa9d8ed00",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b682914d5804238b8f35859c120f106",
            "value": 3
          }
        },
        "45a9f7b861214740b42a2b8f877c5c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d718ee974044ea38de5210962237e60",
            "placeholder": "​",
            "style": "IPY_MODEL_df0034bb5bba47ffa1e5812a45a16da6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4c9ff75fd48547fd93f61ebb13dc0605": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d718ee974044ea38de5210962237e60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53a8acbe80e14b57b39661d9f8169d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "559939daaa40410a842934de61d14d3c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c9d85de11c47e98966562666826d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5705b6cf22fd427fa3355dce6397a688": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45a9f7b861214740b42a2b8f877c5c64",
              "IPY_MODEL_33ee591bba8f4f73ae0a10ce9579a6b3",
              "IPY_MODEL_aa493bb732ba45c2923f96c69fa0521c"
            ],
            "layout": "IPY_MODEL_7155393cd4074d7fbf0d8f928926e5be"
          }
        },
        "5ea357a385ce4d75a3ba03d2c12c7333": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7155393cd4074d7fbf0d8f928926e5be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a61eb2734c44f3b89db68e3c47bf695": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b682914d5804238b8f35859c120f106": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8bf3b5cfa7824aa4a7f9aaaaa9d8ed00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a054f1a4c4d24039917fc38006d17fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1f9b273f94147b1b2c4bf9432193072",
            "placeholder": "​",
            "style": "IPY_MODEL_2137580d9c2a467491aeafb3b5cecce9",
            "value": " 3/3 [00:04&lt;00:00,  1.36s/it]"
          }
        },
        "a5261604be104e0981894d312fbb2f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af5eff91ee7441b5a53890f363e1bad4",
              "IPY_MODEL_2299478f9c1240f1833dbf0e7e9935bd",
              "IPY_MODEL_a054f1a4c4d24039917fc38006d17fa4"
            ],
            "layout": "IPY_MODEL_5ea357a385ce4d75a3ba03d2c12c7333"
          }
        },
        "a67fe0f0f21048749940f3df48e44eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd80bf58df454a6a8dc45285ac03e2f4",
            "placeholder": "​",
            "style": "IPY_MODEL_56c9d85de11c47e98966562666826d4a",
            "value": "ego4d_videos_4.zip: 100%"
          }
        },
        "aa493bb732ba45c2923f96c69fa0521c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_165f599c8b824cf2bc28541c9cf9c9b3",
            "placeholder": "​",
            "style": "IPY_MODEL_be613a10608b46609547210c8f62f271",
            "value": " 3/3 [00:03&lt;00:00,  1.28s/it]"
          }
        },
        "af5eff91ee7441b5a53890f363e1bad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eca46e5c76734f76ab5b35be4a5d5c68",
            "placeholder": "​",
            "style": "IPY_MODEL_4c9ff75fd48547fd93f61ebb13dc0605",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "bd5510db9e904f298e9e720a9ff3bf39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be613a10608b46609547210c8f62f271": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd80bf58df454a6a8dc45285ac03e2f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d249a17f7d1440918f96cb02001d084a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a67fe0f0f21048749940f3df48e44eef",
              "IPY_MODEL_14bef7360ba44509be50d7b5af77f8a3",
              "IPY_MODEL_29524b0bfc6748a59c474228e8e10bf3"
            ],
            "layout": "IPY_MODEL_fdc6f9e4dfa34836870bea6afbb7c441"
          }
        },
        "d50bbac67ddd49d29e919db12443e6d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df0034bb5bba47ffa1e5812a45a16da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e11ea97d34c1402f83f61c6b88e079ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1f9b273f94147b1b2c4bf9432193072": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eca46e5c76734f76ab5b35be4a5d5c68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdc6f9e4dfa34836870bea6afbb7c441": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
